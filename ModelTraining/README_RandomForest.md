# ModelTraining / RandomForest.ipynb â€” ForÃªt AlÃ©atoire

## ğŸ“‹ RÃ©sumÃ© Global

Ce notebook implÃ©mente la **modÃ©lisation prÃ©dictive des prix immobiliers** par Random Forest (ForÃªt AlÃ©atoire), un algorithme d'apprentissage automatique non-paramÃ©trique. Il complÃ¨te l'approche Ã©conomÃ©trique classique (OLS) en capturant automatiquement les non-linÃ©aritÃ©s et interactions entre variables.

### RÃ´le dans le Projet

Le Random Forest reprÃ©sente l'**approche Machine Learning** de la modÃ©lisation des prix. Ce notebook :

1. **Charge** les mÃªmes donnÃ©es que LinearRegression.ipynb
2. **PrÃ©pare** les variantes du dataset (original, log, scaled)
3. **EntraÃ®ne** des modÃ¨les RandomForestRegressor (scikit-learn)
4. **Ã‰value** les performances (RMSE, RÂ², MAE)
5. **Visualise** les prÃ©dictions et l'importance des variables
6. **Compare** avec les rÃ©sultats OLS

### Pourquoi C'est Utile

- **Performance prÃ©dictive** : GÃ©nÃ©ralement RMSE infÃ©rieur Ã  OLS
- **Non-paramÃ©trique** : Pas d'hypothÃ¨se sur la forme fonctionnelle
- **Interactions** : Capture automatiquement les interactions entre variables
- **Robustesse** : RÃ©sistant aux outliers et multicolinÃ©aritÃ©
- **Importance des variables** : Ranking automatique des features

### Comment Il Participe Ã  la DÃ©marche Ã‰conomÃ©trique

```
[Phase 1] DataPreparation.ipynb â†’ df_grenoble_vente.csv
                                           â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â–¼                                      â–¼
[Phase 2a]      LinearRegression.ipynb            RandomForest.ipynb    [Phase 2b]
                (ParamÃ©trique, OLS)                (Non-paramÃ©trique, ML)
                        â”‚                                      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â–¼
                                  Comparaison des modÃ¨les
                                  Trade-off : InterprÃ©tabilitÃ© vs Performance
```

## ğŸ“‚ Contenu du Notebook

### Structure (10 Sections)

1. **Importations et Configuration** - BibliothÃ¨ques Python
2. **Chargement des DonnÃ©es** - df_grenoble_vente.csv
3. **PrÃ©paration des Sous-Ensembles** - SÃ©paration X/y
4. **Fonctions Utilitaires** - Logs et standardisation
5. **Aides de Visualisation** - Fonctions plot
6. **CrÃ©ation des Variantes** - Datasets log/scaled
7. **DÃ©finition du ModÃ¨le** - RandomForestRegressor
8. **EntraÃ®nement Multi-Datasets** - Boucle sur variantes
9. **Visualisations** - ObservÃ© vs prÃ©dit, importances
10. **Conclusion** - SynthÃ¨se

## ğŸŒ³ Random Forest : Principes

### Algorithme

Le Random Forest est un **ensemble de arbres de dÃ©cision** (CART) construits par :

1. **Bootstrap Aggregating (Bagging)** :
   - CrÃ©er B Ã©chantillons bootstrap (avec remplacement) du dataset
   - EntraÃ®ner un arbre sur chaque Ã©chantillon

2. **SÃ©lection AlÃ©atoire de Features** :
   - Ã€ chaque nÅ“ud de chaque arbre, sÃ©lectionner alÃ©atoirement m features parmi p
   - Diviser sur la meilleure feature parmi ces m
   - DÃ©corrÃ¨le les arbres â†’ rÃ©duit la variance

3. **PrÃ©diction par Vote/Moyenne** :
   - RÃ©gression : moyenne des prÃ©dictions de tous les arbres
   - Classification : vote majoritaire

### Formule

```
Å· = (1/B) Ã— Î£(b=1 to B) T_b(x)

oÃ¹ :
  B = nombre d'arbres (ex: 100)
  T_b = prÃ©diction de l'arbre b
  x = vecteur de features
```

### HyperparamÃ¨tres ClÃ©s

| ParamÃ¨tre | Description | Valeur typique | Impact |
|-----------|-------------|----------------|--------|
| `n_estimators` | Nombre d'arbres B | 100 | Plus = meilleur (rendements dÃ©croissants) |
| `max_depth` | Profondeur maximale | None | None = croissance complÃ¨te |
| `min_samples_split` | Min observations pour split | 2 | Plus = arbres plus simples |
| `min_samples_leaf` | Min observations par feuille | 1 | Plus = lissage |
| `max_features` | Features Ã  considÃ©rer par split | 'sqrt' | RÃ©gression: 'sqrt' ou 'log2' |
| `random_state` | Seed alÃ©atoire | 42 | ReproductibilitÃ© |

## ğŸ“¦ Packages UtilisÃ©s

### scikit-learn

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
```

**Fonctions clÃ©s** :
- `RandomForestRegressor()` : Instanciation du modÃ¨le
- `.fit(X_train, y_train)` : EntraÃ®nement
- `.predict(X_test)` : PrÃ©dictions
- `.feature_importances_` : Importance des variables (Gini importance)
- `train_test_split()` : Division train/test (80/20)
- MÃ©triques : RMSE, RÂ², MAE

### Comparaison statsmodels vs sklearn

| Aspect | statsmodels (OLS) | sklearn (RF) |
|--------|-------------------|--------------|
| **Objectif** | InfÃ©rence statistique | PrÃ©diction |
| **Sortie** | Coefs, p-values, IC | PrÃ©dictions |
| **Tests** | t, F, DW, BP | Aucun |
| **HypothÃ¨ses** | LinÃ©aritÃ©, homosc., normalitÃ© | Aucune |
| **InterprÃ©tation** | Î² = effet marginal | Feature importance |
| **Overfitting** | Risque faible (peu de params) | Risque si mal paramÃ©trÃ© |

## ğŸ”„ Workflow EntrÃ©e â†’ Traitement â†’ Sortie

```
INPUT
df_grenoble_vente.csv (1288 obs Ã— 7 vars)
         â”‚
         â”‚ SÃ©paration X/y
         â–¼
X: [surface_bati, nb_pieces, date, surface_terrain, type_local_1234]
y: [price]
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼              â–¼              â–¼
   Dataset ORIG   Dataset LOG    Dataset SCALED  Dataset LOG+SCALED
         â”‚              â”‚              â”‚              â”‚
         â”‚ train_test_split(test_size=0.2, random_state=42)
         â–¼
   X_train (80%)   X_test (20%)
   y_train         y_test
         â”‚              â”‚
         â”‚ RandomForestRegressor(n_estimators=100, random_state=42)
         â”‚
         â–¼
   model.fit(X_train, y_train)
         â”‚
         â”‚ EntraÃ®nement : Construction de 100 arbres CART
         â”‚                 â€¢ Bootstrap sampling
         â”‚                 â€¢ Feature randomization
         â”‚                 â€¢ Croissance jusqu'Ã  puretÃ©
         â–¼
   ModÃ¨le entraÃ®nÃ©
         â”‚
         â”‚ model.predict(X_test)
         â–¼
   y_pred (prÃ©dictions sur test set)
         â”‚
         â”‚ Ã‰valuation
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ©triques                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚ â€¢ RÂ² = 1 - RSS/TSS             â”‚
â”‚ â€¢ RMSE = âˆš(MSE)                â”‚
â”‚ â€¢ MAE = mean(|y - Å·|)          â”‚
â”‚                                â”‚
â”‚ â€¢ Feature importances          â”‚
â”‚   (Gini importance)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
OUTPUT
Tableau comparatif + Graphiques
```

## ğŸ“Š Importance des Variables (Gini Importance)

### Calcul

Pour chaque feature j :
```
Importance(j) = Î£(tous les arbres t) Î£(tous les nÅ“uds n utilisant j) 
                  [RÃ©duction de l'impuretÃ© au nÅ“ud n]
                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                Nombre total d'arbres
```

**ImpuretÃ©** : MSE (Mean Squared Error) pour la rÃ©gression
```
MSE(nÅ“ud) = (1/N) Ã— Î£(i dans nÅ“ud) (y_i - È³_nÅ“ud)Â²
```

**RÃ©duction d'impuretÃ©** lors d'un split :
```
Î”ImpuretÃ© = MSE(parent) - [p_left Ã— MSE(left) + p_right Ã— MSE(right)]

oÃ¹ p_left = proportion d'Ã©chantillons dans le fils gauche
```

### InterprÃ©tation

- **Valeur** : Entre 0 et 1, somme = 1
- **Plus Ã©levÃ©e = plus importante** pour la prÃ©diction
- **Attention** : 
  - Biais vers variables continues (plus de splits possibles)
  - Pas d'infÃ©rence statistique (pas de p-value)
  - Importance relative, pas absolue

### Exemple

```
Feature Importances :
  surface_bati     0.52  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  type_local_1234  0.21  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  nb_pieces        0.15  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  surface_terrain  0.08  â–ˆâ–ˆâ–ˆ
  date             0.04  â–ˆ
```

InterprÃ©tation :
- `surface_bati` explique 52% de la rÃ©duction totale d'impuretÃ©
- C'est la variable la plus prÃ©dictive
- `date` est peu informative (4%)

## ğŸ¯ RÃ©sultats Attendus

### Performance Typique

| MÃ©trique | OLS LinÃ©aire | OLS Log-Log | Random Forest |
|----------|--------------|-------------|---------------|
| **RÂ²** | 0.70-0.75 | 0.75-0.78 | 0.78-0.85 |
| **RMSE** | 55,000-60,000â‚¬ | 50,000-55,000â‚¬ | 45,000-52,000â‚¬ |
| **MAE** | 40,000-45,000â‚¬ | 36,000-40,000â‚¬ | 32,000-38,000â‚¬ |

### Avantages Random Forest

âœ… **Meilleure prÃ©cision** : RMSE typiquement 5-15% infÃ©rieur Ã  OLS  
âœ… **Capture non-linÃ©aritÃ©s** : Effet de seuil, rendements dÃ©croissants  
âœ… **Interactions automatiques** : surface Ã— type_local sans spÃ©cification manuelle  
âœ… **Robustesse** : Peu sensible aux outliers  
âœ… **Pas de multicolinÃ©aritÃ©** : GÃ¨re surface et nb_pieces corrÃ©lÃ©s

### InconvÃ©nients Random Forest

âŒ **BoÃ®te noire** : Difficile d'interprÃ©ter un effet marginal prÃ©cis  
âŒ **Pas de tests statistiques** : Pas de p-value, pas d'intervalle de confiance  
âŒ **Extrapolation** : Ne prÃ©dit pas hors de la plage d'entraÃ®nement  
âŒ **Overfitting** : Si mal paramÃ©trÃ© (arbres trop profonds, peu d'arbres)  
âŒ **Computationnellement coÃ»teux** : EntraÃ®nement plus lent que OLS

## ğŸ” Comparaison avec OLS

### Quand Utiliser OLS ?

- **Objectif** : Comprendre les dÃ©terminants, tester des hypothÃ¨ses
- **InterprÃ©tation** : Besoin d'Ã©lasticitÃ©s, effets marginaux
- **Publication** : Article acadÃ©mique (tests requis)
- **DonnÃ©es** : Relation linÃ©aire plausible
- **Taille** : Petit dataset (n < 100)

### Quand Utiliser Random Forest ?

- **Objectif** : Maximiser la prÃ©cision de prÃ©diction
- **ComplexitÃ©** : Relations non-linÃ©aires, interactions multiples
- **Robustesse** : DonnÃ©es bruitÃ©es, outliers
- **Taille** : Large dataset (n > 500)
- **Application** : Outil opÃ©rationnel (estimation automatique)

### Approche Hybride

```
1. Exploration : Random Forest
   â†’ Identifier les variables importantes
   â†’ DÃ©tecter les interactions significatives

2. Confirmation : OLS
   â†’ Tester ces variables/interactions spÃ©cifiquement
   â†’ Quantifier les effets marginaux
   â†’ Publier les rÃ©sultats interprÃ©tables
```

## ğŸ› ï¸ Instructions d'ExÃ©cution

### PrÃ©requis

Identiques Ã  LinearRegression.ipynb :
1. DonnÃ©es : `../DataPreprocessing/PreprocessedData/df_grenoble_vente.csv`
2. Packages : `pip install pandas numpy scikit-learn matplotlib seaborn`

### ExÃ©cution

```bash
cd ModelTraining
jupyter notebook RandomForest.ipynb
# ExÃ©cuter toutes les cellules
```

**Temps** : ~1-2 minutes (plus long que OLS, entraÃ®nement de 100 arbres)

### Validation

```python
# VÃ©rifier les modÃ¨les entraÃ®nÃ©s
for name, model in models.items():
    r2 = metrics[name]['r2_test']
    rmse = metrics[name]['rmse_test']
    print(f"{name}: RÂ²={r2:.3f}, RMSE={rmse:,.0f}â‚¬")

# Attendu : RÂ² > 0.75, RMSE < 55,000â‚¬
```

## ğŸ“ Concepts Machine Learning

### Bias-Variance Tradeoff

**Erreur totale** = BiaisÂ² + Variance + Erreur irrÃ©ductible

- **Biais** : Erreur due Ã  des hypothÃ¨ses simplificatrices (ex: linÃ©aritÃ© en OLS)
- **Variance** : SensibilitÃ© aux fluctuations des donnÃ©es d'entraÃ®nement
- **Random Forest** : RÃ©duit la variance (bagging) sans augmenter le biais (arbres profonds)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        BIAS-VARIANCE TRADEOFF                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

 Erreur
   ^
   â”‚                     
   â”‚        â€¢            Arbre unique (CART)
   â”‚       / \           Bias faible, Variance Ã‰LEVÃ‰E
   â”‚      /   \          â†’ Overfitting
   â”‚     /  RF \
   â”‚    /  â—    \        Random Forest
   â”‚   /         \       Bias faible, Variance RÃ‰DUITE
   â”‚  /  OLS      \      â†’ Bon Ã©quilibre
   â”‚ /      â—      \
   â”‚/_______________\__  OLS simple
   â”‚                    Bias Ã‰LEVÃ‰, Variance faible
   â”‚                    â†’ Underfitting
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> ComplexitÃ© du modÃ¨le
     Simple                        Complexe
```

### Bagging (Bootstrap Aggregating)

**Principe** : RÃ©duire la variance en moyennant plusieurs modÃ¨les

1. CrÃ©er B Ã©chantillons bootstrap (tirage avec remplacement)
2. EntraÃ®ner un modÃ¨le sur chaque Ã©chantillon
3. PrÃ©dire en moyennant les B prÃ©dictions

**MathÃ©matiquement** :
```
Var(moyenne de B modÃ¨les) â‰ˆ Var(modÃ¨le unique) / B

Si les modÃ¨les sont indÃ©pendants (dÃ©corrÃ©lation par feature randomization)
```

### Out-of-Bag (OOB) Error

Estimation de l'erreur de gÃ©nÃ©ralisation **sans validation croisÃ©e** :

- Chaque arbre est entraÃ®nÃ© sur ~63% des donnÃ©es (bootstrap)
- Les 37% restants = OOB samples pour cet arbre
- PrÃ©dire chaque observation avec les arbres qui ne l'ont pas vue
- Calculer l'erreur sur ces prÃ©dictions OOB

```python
model = RandomForestRegressor(oob_score=True)
model.fit(X, y)
print(f"OOB RÂ²: {model.oob_score_:.3f}")
```

## ğŸ“ SchÃ©mas Explicatifs

### SchÃ©ma : Construction d'un Random Forest

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          CONSTRUCTION D'UN RANDOM FOREST (B=3 arbres)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dataset Original (n=1288 observations)
[Obs1, Obs2, Obs3, ..., Obs1288]
         â”‚
         â”‚ Bootstrap Sampling (avec remplacement)
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼        â–¼        
Bootstrap1  Bootstrap2  Bootstrap3
(n=1288)    (n=1288)    (n=1288)
    â”‚         â”‚          â”‚
    â”‚ Certaines obs rÃ©pÃ©tÃ©es, d'autres absentes (~37% OOB)
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â–¼         â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ Arbre1â”‚ â”‚ Arbre2â”‚  â”‚ Arbre3â”‚
â”‚       â”‚ â”‚       â”‚  â”‚       â”‚
â”‚   â€¢   â”‚ â”‚   â€¢   â”‚  â”‚   â€¢   â”‚ â† NÅ“ud racine
â”‚  / \  â”‚ â”‚  / \  â”‚  â”‚  / \  â”‚
â”‚ â€¢   â€¢ â”‚ â”‚ â€¢   â€¢ â”‚  â”‚ â€¢   â€¢ â”‚ â† NÅ“uds internes
â”‚/ \ / \â”‚ â”‚/ \ / \â”‚  â”‚/ \ / \â”‚   (splits sur features alÃ©atoires)
â”‚â€¢ â€¢ â€¢ â€¢â”‚ â”‚â€¢ â€¢ â€¢ â€¢â”‚  â”‚â€¢ â€¢ â€¢ â€¢â”‚ â† Feuilles (prÃ©dictions)
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜

    â”‚         â”‚          â”‚
    â”‚ PrÃ©diction pour une nouvelle observation x_new
    â”‚
    â–¼         â–¼          â–¼
  250kâ‚¬     280kâ‚¬      265kâ‚¬
    â”‚         â”‚          â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚         â”‚
         â–¼         â–¼
    Moyenne : (250+280+265)/3 = 265kâ‚¬
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                PrÃ©diction finale
```

---

**Version** : 1.0  
**DerniÃ¨re mise Ã  jour** : 13 novembre 2025  
**Auteur** : Projet Ã‰conomÃ©trie AppliquÃ©e
